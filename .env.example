# Environment Configuration for LLM Provider Integration
# Copy this file to .env and configure the values for your setup

# OpenAI Configuration (optional)
# Get your API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# Ollama Configuration (optional)
# Ensure ollama is running locally before using
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:32b

# Development Settings
DEBUG=true
LOG_LEVEL=info

# Application Settings
DEFAULT_LLM_PROVIDER=simple
MAX_LLM_RESPONSE_TIME=300

# Note: At least one LLM provider should be configured for AI-powered recommendations
# The 'simple' provider works without any configuration and uses Phase 1 algorithm