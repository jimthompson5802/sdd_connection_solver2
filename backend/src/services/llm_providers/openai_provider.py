from typing import List, Dict, Any
import time
import importlib
import logging


class OpenAIProvider:
    """Back-compat shim that talks to openai client directly for tests."""

    def __init__(self, api_key: str, model_name: str = "gpt-4o-mini") -> None:
        # api_key kept for signature compatibility; tests patch client
        from src.exceptions import ConfigurationError

        # Validate basic configuration
        if not api_key or not isinstance(api_key, str) or not api_key.strip():
            raise ConfigurationError("OpenAI API key is required", config_key="OPENAI_API_KEY")
        if not model_name or not isinstance(model_name, str) or not model_name.strip():
            raise ConfigurationError("OpenAI model_name is required", config_key="OPENAI_MODEL_NAME")

        self._api_key = api_key
        self._model_name = model_name

    def generate_recommendations(
        self,
        remaining_words: List[str],
        previous_guesses: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        start = time.time()
        # Build prompt. Tests patch clients so content isn't critical.
        prompt = (
            "Given remaining words: "
            + ", ".join(remaining_words)
            + ". Suggest four related words as comma-separated values."
        )

        # Prefer langchain's with_structured_output when available (produces JSON-like output)
        raw: Any = ""
        try:
            # Try langchain LLM first
            from langchain.llms import OpenAI as LangchainOpenAI

            llm = LangchainOpenAI(openai_api_key=self._api_key, model_name=self._model_name)
            if hasattr(llm, "with_structured_output"):
                try:
                    # Use json_mode to request JSON-style output. Schema provided as dict-like hint.
                    structured = llm.with_structured_output(
                        {"recommended_words": list, "connection": str, "explanation": str}, method="json_mode"
                    )
                    # Some wrappers expose invoke/generate â€” try common names
                    # Prefer calling the structured wrapper directly. Try common
                    # compatibility methods (`invoke`, `generate`) then fall back
                    # to calling the wrapper as a callable.
                    if hasattr(structured, "invoke"):
                        raw = structured.invoke(prompt)
                    elif hasattr(structured, "generate"):
                        raw = structured.generate(prompt)
                    else:
                        raw = structured(prompt)
                except Exception:
                    # Fall back to SDK path below
                    raw = ""
                else:
                    # If structured produced a non-string/dict (e.g., MagicMock in tests),
                    # fall back to an empty raw so SDK path is attempted below.
                    if not isinstance(raw, (str, dict, list)):
                        raw = ""
            else:
                raw = ""
        except Exception:
            raw = ""

        # If langchain structured output not available, fall back to OpenAI SDK (patched in tests)
        if not raw:
            openai_mod = importlib.import_module("openai")
            OpenAIClient = getattr(openai_mod, "OpenAI")
            client = OpenAIClient()
            # call to SDK; tests patch OpenAI client
            resp = client.chat.completions.create(
                model=self._model_name, messages=[{"role": "user", "content": prompt}]
            )
            content = getattr(getattr(resp.choices[0], "message"), "content", "")
            raw = content

        def _map_to_original(items: List[str]) -> List[str]:
            mapped: List[str] = []
            for w in items:
                lw = w.lower()
                found = False
                for orig in remaining_words:
                    if orig.lower() == lw:
                        mapped.append(orig)
                        found = True
                        break
                if not found:
                    mapped.append(w)
            return mapped

        # At this point we require the model to return a JSON object (dict).
        # Do not attempt to parse free-form text responses anymore.
        if not isinstance(raw, dict):
            # Some SDK wrappers may return objects with a `content` attribute/string; avoid coercion
            raise ValueError("not json object")

        # Use structured response directly
        parsed = raw
        if not all(k in parsed for k in ("recommended_words", "connection", "explanation")):
            raise ValueError("not json object")

        words_out = [w for w in parsed.get("recommended_words", [])][:4]
        connection_out = str(parsed.get("connection", ""))
        explanation_out = str(parsed.get("explanation", ""))

        generation_time_ms = int((time.time() - start) * 1000)
        # Provide both the new structured keys and legacy compatibility keys
        return {
            "recommended_words": words_out,
            "connection": connection_out,
            "explanation": explanation_out,
            # Legacy compatibility
            "connection_explanation": explanation_out or connection_out,
            "generation_time_ms": generation_time_ms,
        }

    # Back-compat: provide the singular method name used by the rest of the codebase
    def generate_recommendation(self, prompt: str) -> Any:
        """Generate recommendation given a raw prompt string.

        This method is used when the provider is created via the factory and the
        service code passes a prompt. It attempts to call the LangChain wrapper
        with structured output, falling back to the OpenAI SDK path which tests
        commonly patch.
        """
        start = time.time()
        raw: Any = ""
        try:
            from langchain.llms import OpenAI as LangchainOpenAI

            llm = LangchainOpenAI(openai_api_key=self._api_key, model_name=self._model_name)
            if hasattr(llm, "with_structured_output"):
                try:
                    structured = llm.with_structured_output(
                        {"recommended_words": list, "connection": str, "explanation": str}, method="json_mode"
                    )
                    if hasattr(structured, "invoke"):
                        raw = structured.invoke(prompt)
                    elif hasattr(structured, "generate"):
                        raw = structured.generate(prompt)
                    else:
                        raw = structured(prompt)
                except Exception:
                    raw = ""
            else:
                raw = ""
        except Exception:
            raw = ""

        if not raw:
            openai_mod = importlib.import_module("openai")
            OpenAIClient = getattr(openai_mod, "OpenAI")
            client = OpenAIClient()
            resp = client.chat.completions.create(
                model=self._model_name, messages=[{"role": "user", "content": prompt}]
            )
            content = getattr(getattr(resp.choices[0], "message"), "content", "")
            raw = content

        # Enforce JSON/dict contract
        if not isinstance(raw, dict):
            raise ValueError("not json object")

        parsed = raw
        if not all(k in parsed for k in ("recommended_words", "connection", "explanation")):
            raise ValueError("not json object")

        # Return structured dict (same shape as generate_recommendations)
        generation_time_ms = int((time.time() - start) * 1000)
        return {
            "recommended_words": [w for w in parsed.get("recommended_words", [])][:4],
            "connection": str(parsed.get("connection", "")),
            "explanation": str(parsed.get("explanation", "")),
            "connection_explanation": parsed.get("explanation", "") or parsed.get("connection", ""),
            "generation_time_ms": generation_time_ms,
        }


# Minimal logger/metrics shims to satisfy tests that patch them
logger = logging.getLogger(__name__)


class _MetricsShim:
    def increment(self, *args, **kwargs):
        return None

    def timing(self, *args, **kwargs):
        return None


metrics = _MetricsShim()
